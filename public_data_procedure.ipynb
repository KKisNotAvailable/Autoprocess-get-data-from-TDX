{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction\n",
                "The purpose of this file is to get the public transit data from TDX. Since there are multiple steps to get the complete bilateral public travel data, we figured the form of notebook gives clearer deomnstration of the entire process than python script files.\n",
                "\n",
                "Specifically, there are three steps:\n",
                "1. Fetch the data from TDX for all of the bilateral pairs (pairs are split into several files due to API service limitation), and stack the results.\n",
                "2. Check for missing travel times on either going back or forth, should there exist such pairs, collect them and run again with different departure times.\n",
                "3. There will still be some pairs with no entry, and they are treated in two ways: fill with walking time (if their walking time is less than 30 mins) or manually look them up using Google Maps (there shouldn't be many so this is viable).\n",
                "\n",
                "Finally, merge all the data and we will get the public travel time data. Optionally, we can turn the result into a matrix."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Retrieve Data From TDX\n",
                "The routing service can only be access through API calls, and we will roughly introduce the process.\n",
                "First, we acquire an access token, which last for 24 hours, after granted authorization of accessing API with our \"client ID\" and \"client secret\". Then when we access the routing result through API, with this access token attached to the API request.\n",
                "In our implementation, for the purpose of data storage efficiency, with a pair of points A and B, we fetch the results of \"A to B\" and \"B to A\" at the same time and store them in the same row.\n",
                "Therefore, for a pair of points, the code would be simply calling functions like the following block."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Single Pair Example\n",
                "import TDX_retriever as tr\n",
                "import pandas as pd\n",
                "\n",
                "# your info of api\n",
                "client_id = \"your-client-id\"\n",
                "client_secret = \"your-client-secret\"\n",
                "TDX = tr.TDX_retriever(client_id, client_secret)\n",
                "\n",
                "# some centroids as example\n",
                "c1 = [24.9788580602204, 121.55598430669878]\n",
                "c2 = [24.92583789587648, 121.34128216256848]\n",
                "\n",
                "# get routing result from TDX\n",
                "# notice that the returned result is filtered, not original respond\n",
                "single_pair_res = TDX.get_transport_result(c1, c2)\n",
                "\n",
                "# save as a dataframe\n",
                "cols = [\n",
                "    'A_lat', 'A_lon', 'B_lat', 'B_lon',\n",
                "    'AB_travel_time', 'AB_ttl_cost', 'AB_transfer_cnt', 'AB_route',\n",
                "    'BA_travel_time', 'BA_ttl_cost', 'BA_transfer_cnt', 'BA_route'\n",
                "]\n",
                "df = pd.DataFrame(columns=cols)\n",
                "df.loc[len(df)] = single_pair_res\n",
                "\n",
                "# show the df\n",
                "df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For a list of pairs, we call the functions in basically the same way. However, due to the API access limitation, we have to split the list into multiple sub-files. Therefore, when running the code, we need to provide the serial number of the sub-files as the command-line arguments. Another command-line argument required is the path to your API information. The following is an example of command-line for executing the code, using the 7th sub-file.\n",
                "```\n",
                "python TDX_retriever.py your_api_info.json 7\n",
                "```\n",
                "The following block is an example of the layout in the api info json file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "javascript"
                }
            },
            "outputs": [],
            "source": [
                "{\n",
                "    \"client_id\": \"your-client-id\",\n",
                "    \"client_secret\": \"your-client-secret\"\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the actual implementation, we executed the command-line using batch script files (\".bat\" files), rather than typing the above command-line input. As we just mentioned, there are multiple sub-files and such batch files makes rerunning the command much easier. Before an example of the batch file, here are some notes:\n",
                "* Things after \"@REM\" are comments.\n",
                "* In our case, we set the batch file name as \"run_py_key1.bat\" and read the api information from a json file called \"api_key1.json\".\n",
                "* We put the serial number as a command-line argument here, and the batch file will pass this number to the step we execute the code.\n",
                "\n",
                "Batch files are also executed using commmand-line, following our code execution command example previously mentioned, the command to run the batch file is as follows: \n",
                "```\n",
                "run_py_key1.bat 7\n",
                "```\n",
                "The below block of code is an example of our batch file. To run on mac, we need \".sh\" files, which need some modification but the main idea is the same."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "bat"
                }
            },
            "outputs": [],
            "source": [
                "@REM set the file name as a variable, for example, \"fname\" would be \"run_py_key1\"\n",
                "set fname=%~n0\n",
                "@REM %fname:~7,4% is extracting 4 characters,\n",
                "@REM starting from the 8th character from file name \"run_py_key1\"\n",
                "@REM therefore \"%fname:~7,4%\" would be \"key1\"\n",
                "python TDX_retriever.py env\\api_%fname:~7,4%.json %1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Since we use VS code as our main editor, there's a function called \"task\" that makes our lives even easier when it comes to executing batch files. To do this, we need to create a file called \"tasks.json\" and save it in a folder called \".vscode\". Example of the \"tasks.json\" file as the next block."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "javascript"
                }
            },
            "outputs": [],
            "source": [
                "{\n",
                "    \"version\": \"2.0.0\",\n",
                "    \"tasks\": [\n",
                "        {\n",
                "            \"label\": \"get TDX with key 1 on file 7\",\n",
                "            \"type\": \"shell\",\n",
                "            \"command\": \"env\\\\run_py_key1.bat 7\"\n",
                "        },\n",
                "        {\n",
                "            \"label\": \"get TDX with key 1 on file 8\",\n",
                "            \"type\": \"shell\",\n",
                "            \"command\": \"env\\\\run_py_key1.bat 8\"\n",
                "        },\n",
                "        {\n",
                "            \"label\": \"run all\",\n",
                "            \"dependsOn\": [\n",
                "                \"get TDX with key 1 on file 7\",\n",
                "                \"get TDX with key 1 on file 8\"\n",
                "            ],\n",
                "            \"dependsOrder\": \"parallel\",\n",
                "            \"presentation\": {\n",
                "                \"reveal\": \"always\",\n",
                "                \"revealProblems\": \"onProblem\",\n",
                "                \"panel\": \"new\"\n",
                "            }\n",
                "        }\n",
                "    ]\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "FINAL REMARK: there are several useful tools for helping this process in the helper.py, Helper_tdx class."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fill Missing\n",
                "Starting from this section, we will use the code blocks to demonstrate our steps and the actual code execution.\n",
                "\n",
                "The process in this section is as follows:\n",
                "1. Stack data.\n",
                "2. (Optional, missing pair fill) Sometimes TDX would return only a partial of provided pairs, could check with the walking data, which has the full pair list before calibration. Should the missing happens, create rows of those pairs and give missing values to them for later missing fill.\n",
                "3. Keep only the pairs with needed counties.\n",
                "4. (first missing value fill) Fill the missings with walking data if the walking time is less than 30 minutes.\n",
                "5. (second missing value fill) Access TDX with different departure time to fill the remaining missing data.\n",
                "6. (third missing value fill) Get the remaining missing values from Google Maps.\n",
                "\n",
                "The reason for using OSRM walking data to fill the public transit data is because of the limitation of the TDX platform. When the distance of two given points are so close that no public transit is available, TDX would report missing. Besides, since we choose 30 minutes as our settings for the first mile in retrieving data from TDX, filling missings with walking time less than 30 minutes should be a reasonable choice.\n",
                "\n",
                "### Stack results\n",
                "After retrieving data from TDX, there should be multiple result files, so we need to first stack them back to the full list then do the checking."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The file already exists in JJinTP_data_TW/public_data/Scratch/, skipping this step...\n"
                    ]
                }
            ],
            "source": [
                "import helper\n",
                "import os\n",
                "\n",
                "# ======== File and Folder settings ========\n",
                "FOLDER_PUBLIC_RAW = 'JJinTP_data_TW/public_data/Raw/'\n",
                "FOLDER_PUBLIC_SCRATCH = 'JJinTP_data_TW/public_data/Scratch/'\n",
                "FOLDER_PUBLIC_MAIN = 'JJinTP_data_TW/public_data/Main/'\n",
                "\n",
                "# The calibration data keeps only the counties we need\n",
                "FILE_CALIB = \"JJinTP_data_TW/calibration_data_TP.csv\"\n",
                "\n",
                "# This is the full list of all counties in the Taipei Metropolitan,\n",
                "# with the longitude and latitude of the county centroids\n",
                "FILE_VILL_CENTROID = \"JJinTP_data_TW/village_centroid_TP.csv\"\n",
                "\n",
                "# The walking data by OSRM\n",
                "FILE_WALKING = os.path.join(FOLDER_PUBLIC_MAIN, 'travel_walking.csv')\n",
                "\n",
                "hpt = helper.Helper_public_travel(\n",
                "    calib_fpath=FILE_CALIB,\n",
                "    centroid_path=FILE_VILL_CENTROID,\n",
                "    walk_fpath=FILE_WALKING,\n",
                "    public_merged_fname='merged_public.csv'\n",
                ")\n",
                "\n",
                "# For reference, column names in TDX respond:\n",
                "# A_villcode, B_villcode, A_lat, A_lon, B_lat, B_lon,\n",
                "# AB_travel_time, AB_ttl_cost, AB_transfer_cnt, AB_route,\n",
                "# BA_travel_time, BA_ttl_cost, BA_transfer_cnt, BA_route\n",
                "\n",
                "# Stack data, would generate the stacked data to folder\n",
                "# file_cnt = 20 because we split the original list into 20 sub-files.\n",
                "# if want to generate a new file, need to manually remove the file;\n",
                "# otherwise, this step will be skipped.\n",
                "hpt.merge_public_files(\n",
                "    source_path=FOLDER_PUBLIC_RAW,\n",
                "    out_path=FOLDER_PUBLIC_SCRATCH,\n",
                "    start_time='10am', file_cnt=20\n",
                ")  # creates \"merged_public.csv\" in the scratch folder."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Optional missing pair check\n",
                "Occationally, TDX might return less number of results than given. The difference in row counts of input and output files indicates the need for this optional step. Besides, duplicated records were also found.\n",
                "Instead of filling the missing pairs for each sub-files, we do it after stacking all sub-files for checking and filling efficiently."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "There are 40500 missing pairs...\n",
                        "Missing pairs restored.\n"
                    ]
                }
            ],
            "source": [
                "hpt.get_missing_pairs(\n",
                "    stacked_public_path=FOLDER_PUBLIC_SCRATCH,\n",
                "    walking_fpath=FILE_WALKING\n",
                ")  # this function will replace the \"merged_public.csv\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Calibrate the counties\n",
                "We use the list of counties from the calibration data to keep the pairs having both points in the calibrated list."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(776881, 14)\n"
                    ]
                }
            ],
            "source": [
                "# this function will replace the \"merged_public.csv\"\n",
                "hpt.calibrate_counties_used(FOLDER_PUBLIC_SCRATCH)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1st missing fill: rerun TDX\n",
                "This part requires several times of rerun using different departure time. We use batch files (or shell scripts on mac) to run the code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. get 10:30 am\n",
                "hpt.get_rerun_pairs(\n",
                "    data_path=FOLDER_PUBLIC_SCRATCH,\n",
                "    data_fname=\"merged_public.csv\",\n",
                "    out_path=FOLDER_PUBLIC_SCRATCH,\n",
                "    target_time=\"1030am\"\n",
                ")  # generates rerun_TDX_1030am.csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "bat"
                }
            },
            "outputs": [],
            "source": [
                "@REM Batch file: run_py_key1.bat\n",
                "@echo off\n",
                "set fname=%~n0\n",
                "python get_public.py api_%fname:~7,4%.json rerun_TDX_1030am.csv fill_1030am.csv ^\n",
                "    --depart_time \"T10:30:00\" ^\n",
                "    --centroid_path \"JJinTP_data_TW/public_data/Scratch/\" ^\n",
                "    --out_path \"JJinTP_data_TW/public_data/Scratch/\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "shellscript"
                }
            },
            "outputs": [],
            "source": [
                "#!/bin/bash\n",
                "# steps in command-line:\n",
                "# 1. chmod +x ./JJinTP_data_TW/public_data/tdx_api/run_py_key1.sh\n",
                "# 2. ./JJinTP_data_TW/public_data/tdx_api/run_py_key1.sh\n",
                "\n",
                "# Extract filename without extension\n",
                "fname=$(basename \"$0\" .sh)\n",
                "\n",
                "TIME_H=\"10\"\n",
                "TIME_M=\"30\"\n",
                "\n",
                "# Run Python script with formatted parameters\n",
                "python get_public.py \"api_${fname:7:4}.json\" \\\n",
                "    rerun_TDX_${TIME_H}${TIME_M}am.csv fill_${TIME_H}${TIME_M}am.csv \\\n",
                "    --depart_time \"T${TIME_H}:${TIME_M}:00\" \\\n",
                "    --centroid_path \"./JJinTP_data_TW/public_data/Scratch/\" \\\n",
                "    --out_path \"./JJinTP_data_TW/public_data/Scratch/\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. get 11:00am\n",
                "hpt.get_rerun_pairs(\n",
                "    data_path=FOLDER_PUBLIC_SCRATCH,\n",
                "    data_fname=\"fill_1030am.csv\",\n",
                "    out_path=FOLDER_PUBLIC_SCRATCH,\n",
                "    target_time=\"1100am\"\n",
                ")  # generates rerun_TDX_1100am.csv"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2nd missing fill: walking data\n",
                "Fill the pairs that has missing on both directions and walking time less than 30 minutes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hpt.fill_with_walk(\n",
                "    data_path=FOLDER_PUBLIC_SCRATCH,\n",
                "    data_fname=\"fill_1100am.csv\",\n",
                "    t_limit_minute=30,\n",
                "    out_path=FOLDER_PUBLIC_SCRATCH\n",
                ")  # this function generates \"fill_walk.csv\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3rd missing fill: manual check on Google Maps\n",
                "We had only 20 pairs left after the rerun TDX step (with departure time 10:30am and 11:00am), so we start to fill the remaining missing pairs with Google Maps.\n",
                "\n",
                "Notice that there won't be files generated if the file already exist in the folder, since we don't want the manually checked results covered by the accidental click.\n",
                "<!-- \n",
                "Why these pairs could be fixed with Google Maps, but the official routing service could not give a valid respond? We had no idea. However, since Google Maps' public routing function works almost perfectly in Taiwan (from our experience), we think the results should be believable.\n",
                "-->"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "hpt.get_manual_check(\n",
                "    data_path=FOLDER_PUBLIC_SCRATCH,\n",
                "    data_fname=\"fill_1100am.csv\",\n",
                "    out_path=FOLDER_PUBLIC_SCRATCH,\n",
                ")  # generates fill_manual_check.csv"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Merge all the fill files with the main\n",
                "This step patches all the fill files (fill_1030am, fill_1100am, fill_manual_check) to the main data. Notice that some of the pairs might still have missing data in one direction, we will fill these with the travel time of their reverse direction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "There are 0 pairs with missing.\n"
                    ]
                }
            ],
            "source": [
                "# Notice that starting from the merged_public.csv, all of the villcode\n",
                "# pairs are sorted, i.e. the A_villcode will be the smaller one.\n",
                "\n",
                "# the order matters, should follow the steps above\n",
                "fill_list = ['fill_1030am.csv', 'fill_1100am.csv', 'fill_manual_check.csv']\n",
                "\n",
                "hpt.make_public_main(\n",
                "    stacked_public_path=FOLDER_PUBLIC_SCRATCH,\n",
                "    fill_file_path=FOLDER_PUBLIC_SCRATCH,\n",
                "    fill_file_list=fill_list,\n",
                "    out_path=FOLDER_PUBLIC_MAIN\n",
                ")  # generates public_travel_time.csv\n",
                "\n",
                "hpt.update_with_walk(\n",
                "    final_public_fpath=os.path.join(FOLDER_PUBLIC_MAIN, \"public_travel_time.csv\"),\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# This completes the public data part.\n",
                "<!--jupyter nbconvert --to html public_data_procedure.ipynb -->"
            ]
        }
    ],
    "metadata": {
        "author": "Kai-Yuan Ke",
        "date": "2025-02-25",
        "kernelspec": {
            "display_name": "env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
